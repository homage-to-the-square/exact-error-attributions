{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba97c6eb-8eb8-4b78-b450-28c06ea183cc",
   "metadata": {},
   "source": [
    "How the simulation is done:\n",
    "- First, a base simulation is done per sample size.\n",
    "- Then, as needed, more simulations are done by upticking the iteration values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c011f0",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff19b317-ba19-4628-a17f-0ff50d15c9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:49:47.586086Z",
     "iopub.status.busy": "2025-09-18T15:49:47.586086Z",
     "iopub.status.idle": "2025-09-18T15:49:47.589590Z",
     "shell.execute_reply": "2025-09-18T15:49:47.589590Z",
     "shell.execute_reply.started": "2025-09-18T15:49:47.586086Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit, logit\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from ddc_utils import compute_average_jn, is_binomial_data_seperable\n",
    "from jzhou_utils import save_obj_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfdd51",
   "metadata": {},
   "source": [
    "# Hyperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1fc130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:49:48.600312Z",
     "iopub.status.busy": "2025-09-18T15:49:48.599313Z",
     "iopub.status.idle": "2025-09-18T15:49:48.602325Z",
     "shell.execute_reply": "2025-09-18T15:49:48.602325Z",
     "shell.execute_reply.started": "2025-09-18T15:49:48.600312Z"
    }
   },
   "outputs": [],
   "source": [
    "pop_index = 1\n",
    "iter_val = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ae7e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:49:49.073680Z",
     "iopub.status.busy": "2025-09-18T15:49:49.072675Z",
     "iopub.status.idle": "2025-09-18T15:49:49.076150Z",
     "shell.execute_reply": "2025-09-18T15:49:49.076150Z",
     "shell.execute_reply.started": "2025-09-18T15:49:49.073680Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_generator = np.random.default_rng(seed=333 * pop_index + iter_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d053a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:49:50.498018Z",
     "iopub.status.busy": "2025-09-18T15:49:50.498018Z",
     "iopub.status.idle": "2025-09-18T15:49:50.501018Z",
     "shell.execute_reply": "2025-09-18T15:49:50.501018Z",
     "shell.execute_reply.started": "2025-09-18T15:49:50.498018Z"
    }
   },
   "outputs": [],
   "source": [
    "population_size = 100_000\n",
    "number_of_coefficients = 1\n",
    "\n",
    "# num_iters_per_population_for_small_samples = 25_000\n",
    "num_iters_per_population_for_small_samples = 50_000\n",
    "num_iters_per_population_for_large_samples = 10_000\n",
    "small_large_sample_co = 100\n",
    "\n",
    "# biased sampling scheme params:\n",
    "sample_probability_centering = 0.77\n",
    "sample_probability_bias_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26197b3-5f53-4dac-b841-377c94b91820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:50:32.361294Z",
     "iopub.status.busy": "2025-09-18T15:50:32.360133Z",
     "iopub.status.idle": "2025-09-18T15:50:32.364328Z",
     "shell.execute_reply": "2025-09-18T15:50:32.363822Z",
     "shell.execute_reply.started": "2025-09-18T15:50:32.361294Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL_SAMPLE_SIZES = (\n",
    "    # [3, 4, 5] +\n",
    "    [6, 7, 9, 11, 13, 16]\n",
    "    # + [20, 25]\n",
    "    # + [i for i in range(30, 45)]\n",
    "    # + [50, 70, 100, 150, 250, 400, 600, 1000, 1400]\n",
    "    # + [2000, 3000, 5000, 7500, 10_000, 15_000, 20_000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03bb99e-8774-4699-94ed-c7e13d2b7804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:50:33.396753Z",
     "iopub.status.busy": "2025-09-18T15:50:33.395752Z",
     "iopub.status.idle": "2025-09-18T15:50:33.400751Z",
     "shell.execute_reply": "2025-09-18T15:50:33.400751Z",
     "shell.execute_reply.started": "2025-09-18T15:50:33.396753Z"
    }
   },
   "outputs": [],
   "source": [
    "njobs = 3\n",
    "multiprocess_backend = \"loky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747aae1-f7a0-4d3d-b7ae-0c1a5f0d08c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5a80fb-1bc4-4114-b4e6-95f7fd870d6c",
   "metadata": {},
   "source": [
    "# Run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf423c-f86f-4d93-852e-192c32fa219c",
   "metadata": {},
   "source": [
    "## Load Finite Population Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd16c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:50:35.394635Z",
     "iopub.status.busy": "2025-09-18T15:50:35.393121Z",
     "iopub.status.idle": "2025-09-18T15:50:35.401121Z",
     "shell.execute_reply": "2025-09-18T15:50:35.401121Z",
     "shell.execute_reply.started": "2025-09-18T15:50:35.394635Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle_filename = f'base_population_data_Logit_1.pickle'\n",
    "pop_data = pd.read_pickle(pickle_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b506e88",
   "metadata": {},
   "source": [
    "## get population-level statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b059d8c-9fa7-4a8f-bcf7-5f84ce118798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:50:36.339401Z",
     "iopub.status.busy": "2025-09-18T15:50:36.338400Z",
     "iopub.status.idle": "2025-09-18T15:50:36.378667Z",
     "shell.execute_reply": "2025-09-18T15:50:36.378667Z",
     "shell.execute_reply.started": "2025-09-18T15:50:36.339401Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [f'x_{i}' for i in range(number_of_coefficients)]\n",
    "pop_x = pop_data[feature_cols]\n",
    "pop_y = pop_data[\"y\"]\n",
    "\n",
    "pop_model = sm.Logit(endog=pop_y, exog=pop_x).fit(disp=0)\n",
    "pop_beta = np.array(pop_model.params)\n",
    "pop_gs = pop_x * (\n",
    "    np.array(pop_y).reshape((population_size, 1))\n",
    "    - np.array(pop_model.predict()).reshape((population_size, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e44da0",
   "metadata": {},
   "source": [
    "## Function to parallelize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745b72c7-d886-4c27-9460-5b198291bad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:50:38.069670Z",
     "iopub.status.busy": "2025-09-18T15:50:38.068662Z",
     "iopub.status.idle": "2025-09-18T15:50:38.074567Z",
     "shell.execute_reply": "2025-09-18T15:50:38.074567Z",
     "shell.execute_reply.started": "2025-09-18T15:50:38.069670Z"
    }
   },
   "outputs": [],
   "source": [
    "def fn_to_parallel(pop_data, temp_sample_size):\n",
    "    obtained_valid_sample = False\n",
    "    non_separable_count = 0\n",
    "    while not obtained_valid_sample:\n",
    "        # intended sample:\n",
    "        pop_data[\"r0\"] = 0\n",
    "        pop_data.loc[\n",
    "            np.random.choice(pop_data.index, size=temp_sample_size, replace=False),\n",
    "            \"r0\",\n",
    "        ] = 1\n",
    "        full_sampled_data = pop_data[pop_data[\"r0\"] == 1]\n",
    "\n",
    "        # biased sample:\n",
    "        pop_data[\"r\"] = 0\n",
    "        marginal_probabilities = expit(\n",
    "            logit(sample_probability_centering)\n",
    "            + sample_probability_bias_factor\n",
    "            * (2 * full_sampled_data[\"y\"] - 1)\n",
    "            * full_sampled_data[\"x_0\"]\n",
    "        )\n",
    "        other_sample_indices = marginal_probabilities.index[\n",
    "            rand_generator.binomial(n=1, p=marginal_probabilities) == 1\n",
    "        ]\n",
    "        pop_data.loc[other_sample_indices, \"r\"] = 1\n",
    "\n",
    "        # sample_data here means the biased sample data.\n",
    "        sample_data = pop_data[pop_data[\"r\"] == 1]\n",
    "\n",
    "        # if the sample size is too small, check for seperability:\n",
    "        realised_sample_size = len(other_sample_indices)\n",
    "        if realised_sample_size < 1_000:\n",
    "            if is_binomial_data_seperable(sample_data, \"y\", \"x_0\"):\n",
    "                non_separable_count = non_separable_count + 1\n",
    "                continue\n",
    "\n",
    "        obtained_valid_sample = True\n",
    "\n",
    "    \"\"\"\n",
    "        Then, compute the logistic betas, ddc, Jns:\n",
    "    \"\"\"\n",
    "    # compute biased x, y, model, beta\n",
    "    sample_x, sample_y = sample_data[feature_cols], sample_data[\"y\"]\n",
    "    sample_beta = np.array(\n",
    "        sm.Logit(endog=sample_y, exog=sample_x).fit(disp=0, maxiter=5_00).params\n",
    "    )\n",
    "    sample_r = pop_data[\"r\"]\n",
    "\n",
    "    # compute full x, y, model, beta\n",
    "    sample_x_full, sample_y_full = (\n",
    "        full_sampled_data[feature_cols],\n",
    "        full_sampled_data[\"y\"],\n",
    "    )\n",
    "    sample_beta_full = np.array(\n",
    "        sm.Logit(endog=sample_y_full, exog=sample_x_full)\n",
    "        .fit(disp=0, maxiter=5_00)\n",
    "        .params\n",
    "    )\n",
    "    sample_r_full = pop_data[\"r0\"]\n",
    "\n",
    "    # ret: sample beta, sample ddc, sample Jn, sample size;\n",
    "    #    intended beta, intended ddc, intended Jn, non seperable count\n",
    "    return (\n",
    "        pd.Series(sample_beta),\n",
    "        pop_gs.corrwith(sample_r)[[\"x_0\"]],\n",
    "        compute_average_jn(\n",
    "            pop_beta, sample_beta, sample_x, sample_y, link_fn=\"Logit\"\n",
    "        ),\n",
    "        realised_sample_size,\n",
    "        \n",
    "        pd.Series(sample_beta_full),\n",
    "        pop_gs.corrwith(sample_r_full)[[\"x_0\"]],\n",
    "        compute_average_jn(\n",
    "            pop_beta,\n",
    "            sample_beta_full,\n",
    "            sample_x_full,\n",
    "            sample_y_full,\n",
    "            link_fn=\"Logit\",\n",
    "        ),\n",
    "        non_separable_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc07af7-e731-4ca3-8c2b-abdfd9266f97",
   "metadata": {},
   "source": [
    "## iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60751ccc-86ea-4876-872e-25ae30ed546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_specific_non_separable_count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fd510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:50:42.692545Z",
     "iopub.status.busy": "2025-09-18T15:50:42.691029Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]\n",
      "\u001b[A [00:00, ?it/s]\n",
      "\u001b[A2it [00:10, 186.69it/s]\n",
      "\u001b[A0it [00:20, 196.64it/s]\n",
      "\u001b[A7it [00:30, 198.41it/s]\n",
      "\u001b[A2it [00:40, 197.34it/s]\n",
      "\u001b[A0it [00:50, 195.50it/s]\n",
      "\u001b[A53it [01:01, 190.40it/s]\n",
      "\u001b[A63it [01:11, 186.76it/s]\n",
      "\u001b[A78it [01:21, 187.92it/s]\n",
      "\u001b[A98it [01:31, 188.92it/s]\n",
      "\u001b[A10it [01:41, 187.16it/s]\n",
      "\u001b[A90it [01:52, 187.36it/s]\n",
      "\u001b[A69it [02:02, 187.42it/s]\n",
      "\u001b[A14it [02:12, 189.35it/s]\n",
      "\u001b[A30it [02:22, 191.87it/s]\n",
      "\u001b[A07it [02:33, 188.18it/s]\n",
      "\u001b[A86it [02:43, 187.92it/s]\n",
      "\u001b[A59it [02:53, 185.53it/s]\n",
      "\u001b[A98it [03:03, 184.92it/s]\n",
      "\u001b[A82it [03:13, 188.43it/s]\n",
      "\u001b[A50it [03:25, 183.77it/s]\n",
      "\u001b[A90it [03:35, 178.32it/s]\n",
      "\u001b[A22it [03:45, 176.74it/s]\n",
      "\u001b[A51it [03:55, 174.69it/s]\n",
      "\u001b[A18it [04:06, 179.90it/s]\n",
      "\u001b[A40it [04:16, 179.32it/s]\n",
      "50000it [04:30, 184.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of non-seperable samples for sample size 6: 65808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                     | 1/6 [04:33<22:49, 273.94s/it]\n",
      "\u001b[A [00:00, ?it/s]\n",
      "\u001b[A5it [00:10, 197.60it/s]\n",
      "\u001b[A5it [00:20, 202.94it/s]\n",
      "\u001b[A1it [00:30, 208.45it/s]\n",
      "\u001b[A1it [00:40, 210.15it/s]\n",
      "\u001b[A29it [00:51, 201.29it/s]\n",
      "\u001b[A03it [01:03, 188.15it/s]\n",
      "\u001b[A31it [01:13, 188.83it/s]\n",
      "\u001b[A91it [01:23, 196.53it/s]"
     ]
    }
   ],
   "source": [
    "for temp_sample_size in tqdm(ALL_SAMPLE_SIZES):\n",
    "    # set up how much to sample for this population:\n",
    "    if temp_sample_size < small_large_sample_co:\n",
    "        num_iters_per_population = num_iters_per_population_for_small_samples\n",
    "    else:\n",
    "        num_iters_per_population = num_iters_per_population_for_large_samples\n",
    "\n",
    "    # run all the results, with the function to parallel above!\n",
    "    agg_results = list(\n",
    "        tqdm(\n",
    "            Parallel(n_jobs=njobs, backend=multiprocess_backend, return_as=\"generator\")(\n",
    "                delayed(fn_to_parallel)(pop_data, temp_sample_size)\n",
    "                for rep in range(num_iters_per_population)\n",
    "            ),\n",
    "            mininterval=10,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sample_specific_non_separable_count[temp_sample_size] = np.sum(\n",
    "        pd.Series([temp_res[7] for temp_res in agg_results])\n",
    "    )\n",
    "    print(\n",
    "        f\"# of non-seperable samples for sample size {temp_sample_size}: {sample_specific_non_separable_count[temp_sample_size]}\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "        Save the data!\n",
    "    \"\"\"\n",
    "    # concatenate the biased versions:\n",
    "    temp_samp_beta_biased = pd.Series([temp_res[0][0] for temp_res in agg_results])\n",
    "    temp_ddc_biased = pd.Series([temp_res[1].iloc[0] for temp_res in agg_results])\n",
    "    temp_jn_biased = pd.Series([temp_res[2][0].iloc[0] for temp_res in agg_results])\n",
    "    realised_sizes = pd.Series([temp_res[3] for temp_res in agg_results])\n",
    "\n",
    "    # concat the SRS versions:\n",
    "    temp_samp_beta_full = pd.Series([temp_res[4][0] for temp_res in agg_results])\n",
    "    temp_ddc_full = pd.Series([temp_res[5].iloc[0] for temp_res in agg_results])\n",
    "    temp_jn_full = pd.Series([temp_res[6][0].iloc[0] for temp_res in agg_results])\n",
    "\n",
    "    temp_ss_data = pd.concat(\n",
    "        [\n",
    "            temp_samp_beta_biased,\n",
    "            temp_ddc_biased,\n",
    "            temp_jn_biased,\n",
    "            realised_sizes,\n",
    "            temp_samp_beta_full,\n",
    "            temp_ddc_full,\n",
    "            temp_jn_full,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    temp_ss_data.columns = [\n",
    "        \"samp_biased\",\n",
    "        \"ddc_biased\",\n",
    "        \"jn_biased\",\n",
    "        \"realized_size_biased\",\n",
    "        \"samp_intended\",\n",
    "        \"ddc_intended\",\n",
    "        \"jn_intended\",\n",
    "    ]\n",
    "\n",
    "    temp_ss_data[\"sample_size\"] = temp_sample_size\n",
    "    temp_ss_data[\"pop_beta\"] = pop_beta[0]\n",
    "\n",
    "    temp_ss_data[\"mse_biased\"] = (\n",
    "        temp_ss_data[\"pop_beta\"] - temp_ss_data[\"samp_biased\"]\n",
    "    ) ** 2\n",
    "\n",
    "    temp_ss_data[\"mse_intended\"] = (\n",
    "        temp_ss_data[\"pop_beta\"] - temp_ss_data[\"samp_intended\"]\n",
    "    ) ** 2\n",
    "\n",
    "    save_obj_pickle(\n",
    "        f\"sim_results/sim_{temp_sample_size}_iter_{iter_val}.pickle\", temp_ss_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50f74a-8f5c-4741-bedd-93ad9cbd4969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
